{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import asyncpraw\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Replace with your actual credentials\n",
        "client_id = ''\n",
        "client_secret = '='\n",
        "user_agent = 'MyRedditApp/0.1 by your_username'\n",
        "\n",
        "reddit = praw.Reddit(client_id=client_id,\n",
        "                     client_secret=client_secret,\n",
        "                     user_agent=user_agent)\n"
      ],
      "metadata": {
        "id": "zwe0QDqG8gcO"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the nest_asyncio patch to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def collect_reddit_comments(subreddit_name, keyword, limit=1000):\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent\n",
        "    )\n",
        "\n",
        "    subreddit = await reddit.subreddit(subreddit_name)\n",
        "    comments = []\n",
        "    count = 0\n",
        "    after = None\n",
        "\n",
        "    while len(comments) < limit:\n",
        "        try:\n",
        "            async for submission in subreddit.search(keyword, limit=None, params={'after': after}):\n",
        "                await submission.load()\n",
        "                submission.comment_limit = 0\n",
        "                submission.comments.replace_more(limit=0)\n",
        "\n",
        "                for comment in submission.comments.list():\n",
        "                    if isinstance(comment, asyncpraw.models.Comment):\n",
        "                        author_name = comment.author.name if comment.author else '[deleted]'\n",
        "                        comments.append([comment.body, author_name, comment.created_utc])\n",
        "                        count += 1\n",
        "\n",
        "                        if count >= limit:\n",
        "                            break\n",
        "\n",
        "                after = submission.id  # Set the 'after' parameter for pagination\n",
        "\n",
        "                if count >= limit:\n",
        "                    break\n",
        "\n",
        "            if count >= limit:\n",
        "                break\n",
        "\n",
        "        except asyncpraw.exceptions.APIException as e:\n",
        "            print(f\"API exception occurred: {e}\")\n",
        "            wait_time = 60  # Wait for 1 minute before retrying\n",
        "            print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "\n",
        "    return comments[:limit]  # Return up to 'limit' number of comments\n",
        "\n",
        "async def main():\n",
        "    comments = await collect_reddit_comments('sarcasm', 'sarcastic', limit=5000)  # Adjust limit as needed\n",
        "    df = pd.DataFrame(comments, columns=['comment', 'author', 'created_utc'])\n",
        "    df.to_csv('reddit_comments.csv', index=False)\n",
        "    print(f\"Total comments collected: {len(df)}\")\n",
        "    print(df.head())\n",
        "\n",
        "# Run the main function\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSMUo68bF1Mr",
        "outputId": "88674fb1-cdfd-4e42-9fee-6fc36e02c8e2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-22c2abee0a60>:21: RuntimeWarning: coroutine 'CommentForest.replace_more' was never awaited\n",
            "  submission.comments.replace_more(limit=0)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b60858edbd0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b6085c0c640>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total comments collected: 5000\n",
            "                                             comment                author  \\\n",
            "0  Woops, I dropped my monster condom for my magn...               manwae1   \n",
            "1                                 That's disgusting.  DelightfulHelper9204   \n",
            "2  There was an episode of Always Sunny where Fra...    Either-Computer635   \n",
            "3  It only works if theyre used. That's how they ...            shits4gigs   \n",
            "4                            Try doing it ironically     East_Bicycle_9283   \n",
            "\n",
            "    created_utc  \n",
            "0  1.719068e+09  \n",
            "1  1.719052e+09  \n",
            "2  1.719074e+09  \n",
            "3  1.719117e+09  \n",
            "4  1.719065e+09  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def clean_comment(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'/u/\\w+', '', text)  # Remove user mentions\n",
        "    text = re.sub(r'r/\\w+', '', text)  # Remove subreddit mentions\n",
        "    text = re.sub(r'\\n', ' ', text)  # Remove newlines\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    return text.lower()\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('reddit_comments.csv')\n",
        "df['cleaned_comment'] = df['comment'].apply(clean_comment)\n",
        "\n",
        "# Manually label the data (for demonstration, we'll assume labels are provided)\n",
        "# 0 for non-sarcastic, 1 for sarcastic\n",
        "labels = [0, 1] * (len(df) // 2)\n",
        "if len(labels) < len(df):\n",
        "    labels.append(0)  # Add one more label to match the length\n",
        "\n",
        "df['label'] = labels\n",
        "\n",
        "# Remove rows with empty or NaN comments\n",
        "df = df.dropna(subset=['cleaned_comment'])\n",
        "df = df[df['cleaned_comment'].str.strip() != '']\n",
        "\n",
        "# Save the labeled data\n",
        "df.to_csv('labeled_reddit_comments.csv', index=False)\n"
      ],
      "metadata": {
        "id": "0DbgNpJ6-VK8"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load labeled data\n",
        "df = pd.read_csv('labeled_reddit_comments.csv')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_comment'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with a Random Forest classifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'tfidf__max_features': [10000, 20000, None],  # Adjusted limit for max_features\n",
        "    'clf__n_estimators': [50, 100],\n",
        "    'clf__max_depth': [None, 10],\n",
        "    'clf__min_samples_split': [2, 5],\n",
        "    'clf__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, error_score='raise')\n",
        "grid_search.fit(X_train, y_train)  # Pass X_train directly here\n",
        "\n",
        "# Evaluate the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG77rnQ9D2Ky",
        "outputId": "403933c4-52cb-44a9-a64e-7043b68943a1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b6085f9da20>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9746707193515705\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98       522\n",
            "           1       0.97      0.98      0.97       465\n",
            "\n",
            "    accuracy                           0.97       987\n",
            "   macro avg       0.97      0.97      0.97       987\n",
            "weighted avg       0.97      0.97      0.97       987\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'trained_model.pkl')\n",
        "\n",
        "# Save the TfidfVectorizer\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"Model and vectorizer saved as 'trained_model.pkl' and 'tfidf_vectorizer.pkl' respectively.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvwYeqLWHeqB",
        "outputId": "30becda7-b4b3-46f1-cb39-6a16263079e1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer saved as 'trained_model.pkl' and 'tfidf_vectorizer.pkl' respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "# Load the trained model and vectorizer\n",
        "model = joblib.load('trained_model.pkl')  # Replace with your actual model file\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Replace with your actual vectorizer file\n",
        "\n",
        "# Sample text to test\n",
        "sample_text = \"Sure, because taking advice from a microwave manual is exactly how I planned my day.\"\n",
        "\n",
        "# Replace with your actual preprocessing steps\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char.isalnum() or char == ' '])  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Preprocess and vectorize the sample text\n",
        "preprocessed_text = preprocess_text(sample_text)\n",
        "X_sample = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Predict using the model\n",
        "prediction = model.predict(X_sample)[0]\n",
        "\n",
        "# Interpret the prediction\n",
        "if prediction == 1:\n",
        "    print(f\"The text '{sample_text}' is sarcastic.\")\n",
        "else:\n",
        "    print(f\"The text '{sample_text}' is not sarcastic.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS3_jKusMbsw",
        "outputId": "cd3823fc-0aba-4f27-f317-97e03dadd6eb"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text 'Sure, because taking advice from a microwave manual is exactly how I planned my day.' is sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('trained_model.pkl')\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "async def fetch_reddit_post_comments(submission_url, limit=5):\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent\n",
        "    )\n",
        "\n",
        "    submission = await reddit.submission(url=submission_url)\n",
        "    await submission.load()\n",
        "\n",
        "    comments = []\n",
        "    async for comment in submission.comments:\n",
        "        if isinstance(comment, asyncpraw.models.Comment):\n",
        "            comments.append(comment.body)\n",
        "            if len(comments) >= limit:\n",
        "                break\n",
        "\n",
        "    return comments[:limit]\n",
        "\n",
        "async def main():\n",
        "    submission_url = 'https://www.reddit.com/r/pics/comments/1dq9ats/after_the_presidential_debate_joe_biden_greeted/'\n",
        "    comments = await fetch_reddit_post_comments(submission_url, limit=10)\n",
        "\n",
        "    # Preprocess comments and predict with the model\n",
        "    preprocessed_comments = [preprocess_text(comment) for comment in comments]\n",
        "    X_comments = vectorizer.transform(preprocessed_comments)\n",
        "    predictions = model.predict(X_comments)\n",
        "\n",
        "    # Filter sarcastic comments\n",
        "    sarcastic_comments = [comments[i] for i in range(len(comments)) if predictions[i] == 1]\n",
        "\n",
        "    print(\"Top 10 comments:\")\n",
        "    for idx, comment in enumerate(comments, start=1):\n",
        "        print(f\"{idx}. {comment}\")\n",
        "\n",
        "    print(\"\\nSarcastic comments detected:\")\n",
        "    for idx, sarcastic_comment in enumerate(sarcastic_comments, start=1):\n",
        "        print(f\"{idx}. {sarcastic_comment}\")\n",
        "\n",
        "# Function to preprocess text (adjust based on your preprocessing steps)\n",
        "def preprocess_text(text):\n",
        "    # Example: lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char.isalnum() or char == ' '])  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Run the main function\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MN4XhNXMfEH",
        "outputId": "0b70b483-7543-4024-f650-1e4b59a52b2a"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 comments:\n",
            "1. It looks like this post is about Politics. Various methods of filtering out content relating to Politics can be found [here](https://www.reddit.com/r/pics/wiki/v2/resources/filter/politics).\n",
            "\n",
            "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/pics) if you have any questions or concerns.*\n",
            "2. The debate was a national embarrassment\n",
            "3. Why can't I see the other comments\n",
            "4. They should be sitting on a porch playing with grandchildren not running a country. Where are the young political leaders? Or even the 60 year olds?\n",
            "5. Politics aside, I think we can all agree last nights debate was an utter shambles and if this is the best we can come up with out of 333M Americans, then god help us.\n",
            "\n",
            "Edit. The “god” part is clearly a figure of speech people, please chill.\n",
            "6. I still can’t believe these are our two options\n",
            "7. This is all just so incredibly sad.\n",
            "8. She needed to help him down the stairs (I wish I was joking about that).\n",
            "9. The best way to deal with this situation is to tell people that they did not see what they just saw.\n",
            "10. It's unfortunate that the quote of the night come from Trump: \"I really don't know what he said at the end of that sentence and I don't think he knows what he said either.\"\n",
            "\n",
            "Sarcastic comments detected:\n",
            "1. Why can't I see the other comments\n",
            "2. Politics aside, I think we can all agree last nights debate was an utter shambles and if this is the best we can come up with out of 333M Americans, then god help us.\n",
            "\n",
            "Edit. The “god” part is clearly a figure of speech people, please chill.\n",
            "3. This is all just so incredibly sad.\n",
            "4. She needed to help him down the stairs (I wish I was joking about that).\n",
            "5. The best way to deal with this situation is to tell people that they did not see what they just saw.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-107-bf968a23e18f>:25: DeprecationWarning: Using CommentForest as an asynchronous iterator has been deprecated and will be removed in a future version.\n",
            "  comments = await fetch_reddit_post_comments(submission_url, limit=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hA4KYe9RSkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}