{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import asyncpraw\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Replace with your actual credentials\n",
        "client_id = ''\n",
        "client_secret = '='\n",
        "user_agent = 'MyRedditApp/0.1 by your_username'\n",
        "\n",
        "reddit = praw.Reddit(client_id=client_id,\n",
        "                     client_secret=client_secret,\n",
        "                     user_agent=user_agent)\n"
      ],
      "metadata": {
        "id": "zwe0QDqG8gcO"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the nest_asyncio patch to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def collect_reddit_comments(subreddit_name, keyword, limit=1000):\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent\n",
        "    )\n",
        "\n",
        "    subreddit = await reddit.subreddit(subreddit_name)\n",
        "    comments = []\n",
        "    count = 0\n",
        "    after = None\n",
        "\n",
        "    while len(comments) < limit:\n",
        "        try:\n",
        "            async for submission in subreddit.search(keyword, limit=None, params={'after': after}):\n",
        "                await submission.load()\n",
        "                submission.comment_limit = 0\n",
        "                submission.comments.replace_more(limit=0)\n",
        "\n",
        "                for comment in submission.comments.list():\n",
        "                    if isinstance(comment, asyncpraw.models.Comment):\n",
        "                        author_name = comment.author.name if comment.author else '[deleted]'\n",
        "                        comments.append([comment.body, author_name, comment.created_utc])\n",
        "                        count += 1\n",
        "\n",
        "                        if count >= limit:\n",
        "                            break\n",
        "\n",
        "                after = submission.id  # Set the 'after' parameter for pagination\n",
        "\n",
        "                if count >= limit:\n",
        "                    break\n",
        "\n",
        "            if count >= limit:\n",
        "                break\n",
        "\n",
        "        except asyncpraw.exceptions.APIException as e:\n",
        "            print(f\"API exception occurred: {e}\")\n",
        "            wait_time = 60  # Wait for 1 minute before retrying\n",
        "            print(f\"Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "\n",
        "    return comments[:limit]  # Return up to 'limit' number of comments\n",
        "\n",
        "async def main():\n",
        "    comments = await collect_reddit_comments('sarcasm', 'sarcastic', limit=5000)  # Adjust limit as needed\n",
        "    df = pd.DataFrame(comments, columns=['comment', 'author', 'created_utc'])\n",
        "    df.to_csv('reddit_comments.csv', index=False)\n",
        "    print(f\"Total comments collected: {len(df)}\")\n",
        "    print(df.head())\n",
        "\n",
        "# Run the main function\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSMUo68bF1Mr",
        "outputId": "88674fb1-cdfd-4e42-9fee-6fc36e02c8e2"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-22c2abee0a60>:21: RuntimeWarning: coroutine 'CommentForest.replace_more' was never awaited\n",
            "  submission.comments.replace_more(limit=0)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b60858edbd0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b6085c0c640>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total comments collected: 5000\n",
            "                                             comment                author  \\\n",
            "0  Woops, I dropped my monster condom for my magn...               manwae1   \n",
            "1                                 That's disgusting.  DelightfulHelper9204   \n",
            "2  There was an episode of Always Sunny where Fra...    Either-Computer635   \n",
            "3  It only works if theyre used. That's how they ...            shits4gigs   \n",
            "4                            Try doing it ironically     East_Bicycle_9283   \n",
            "\n",
            "    created_utc  \n",
            "0  1.719068e+09  \n",
            "1  1.719052e+09  \n",
            "2  1.719074e+09  \n",
            "3  1.719117e+09  \n",
            "4  1.719065e+09  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def clean_comment(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'/u/\\w+', '', text)  # Remove user mentions\n",
        "    text = re.sub(r'r/\\w+', '', text)  # Remove subreddit mentions\n",
        "    text = re.sub(r'\\n', ' ', text)  # Remove newlines\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    return text.lower()\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('reddit_comments.csv')\n",
        "df['cleaned_comment'] = df['comment'].apply(clean_comment)\n",
        "\n",
        "# Manually label the data (for demonstration, we'll assume labels are provided)\n",
        "# 0 for non-sarcastic, 1 for sarcastic\n",
        "labels = [0, 1] * (len(df) // 2)\n",
        "if len(labels) < len(df):\n",
        "    labels.append(0)  # Add one more label to match the length\n",
        "\n",
        "df['label'] = labels\n",
        "\n",
        "# Remove rows with empty or NaN comments\n",
        "df = df.dropna(subset=['cleaned_comment'])\n",
        "df = df[df['cleaned_comment'].str.strip() != '']\n",
        "\n",
        "# Save the labeled data\n",
        "df.to_csv('labeled_reddit_comments.csv', index=False)\n"
      ],
      "metadata": {
        "id": "0DbgNpJ6-VK8"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load labeled data\n",
        "df = pd.read_csv('labeled_reddit_comments.csv')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_comment'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a pipeline with a Random Forest classifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'tfidf__max_features': [10000, 20000, None],  # Adjusted limit for max_features\n",
        "    'clf__n_estimators': [50, 100],\n",
        "    'clf__max_depth': [None, 10],\n",
        "    'clf__min_samples_split': [2, 5],\n",
        "    'clf__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, error_score='raise')\n",
        "grid_search.fit(X_train, y_train)  # Pass X_train directly here\n",
        "\n",
        "# Evaluate the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG77rnQ9D2Ky",
        "outputId": "403933c4-52cb-44a9-a64e-7043b68943a1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b6085f9da20>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9746707193515705\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98       522\n",
            "           1       0.97      0.98      0.97       465\n",
            "\n",
            "    accuracy                           0.97       987\n",
            "   macro avg       0.97      0.97      0.97       987\n",
            "weighted avg       0.97      0.97      0.97       987\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Example: Assuming 'model' and 'vectorizer' are already trained and initialized\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'trained_model.pkl')\n",
        "\n",
        "# Save the TfidfVectorizer\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"Model and vectorizer saved as 'trained_model.pkl' and 'tfidf_vectorizer.pkl' respectively.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvwYeqLWHeqB",
        "outputId": "30becda7-b4b3-46f1-cb39-6a16263079e1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer saved as 'trained_model.pkl' and 'tfidf_vectorizer.pkl' respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "# Assuming you have trained and saved your model and vectorizer\n",
        "# Load the trained model and vectorizer\n",
        "model = joblib.load('trained_model.pkl')  # Replace with your actual model file\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Replace with your actual vectorizer file\n",
        "\n",
        "# Sample text to test\n",
        "sample_text = \"Sure, because taking advice from a microwave manual is exactly how I planned my day.\"\n",
        "\n",
        "# Preprocess the sample text (assuming you have a function for this)\n",
        "# Replace with your actual preprocessing steps\n",
        "def preprocess_text(text):\n",
        "    # Example: lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char.isalnum() or char == ' '])  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Preprocess and vectorize the sample text\n",
        "preprocessed_text = preprocess_text(sample_text)\n",
        "X_sample = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Predict using the model\n",
        "prediction = model.predict(X_sample)[0]\n",
        "\n",
        "# Interpret the prediction\n",
        "if prediction == 1:\n",
        "    print(f\"The text '{sample_text}' is sarcastic.\")\n",
        "else:\n",
        "    print(f\"The text '{sample_text}' is not sarcastic.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS3_jKusMbsw",
        "outputId": "cd3823fc-0aba-4f27-f317-97e03dadd6eb"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text 'Sure, because taking advice from a microwave manual is exactly how I planned my day.' is sarcastic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('trained_model.pkl')  # Replace with your actual model file\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Replace with your actual vectorizer file\n",
        "\n",
        "async def fetch_reddit_post_comments(submission_url, limit=10):\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        user_agent=user_agent\n",
        "    )\n",
        "\n",
        "    submission = await reddit.submission(url=submission_url)\n",
        "    await submission.load()\n",
        "\n",
        "    comments = []\n",
        "    async for comment in submission.comments:\n",
        "        if isinstance(comment, asyncpraw.models.Comment):\n",
        "            comments.append(comment.body)\n",
        "            if len(comments) >= limit:\n",
        "                break\n",
        "\n",
        "    return comments[:limit]\n",
        "\n",
        "async def main():\n",
        "    submission_url = 'https://www.reddit.com/r/IndiaSpeaks/comments/1bcyl4v/whats_wrong_with_india/'\n",
        "    comments = await fetch_reddit_post_comments(submission_url, limit=10)\n",
        "\n",
        "    # Preprocess comments and predict with the model\n",
        "    preprocessed_comments = [preprocess_text(comment) for comment in comments]\n",
        "    X_comments = vectorizer.transform(preprocessed_comments)\n",
        "    predictions = model.predict(X_comments)\n",
        "\n",
        "    # Filter sarcastic comments\n",
        "    sarcastic_comments = [comments[i] for i in range(len(comments)) if predictions[i] == 1]\n",
        "\n",
        "    print(\"Top 10 comments:\")\n",
        "    for idx, comment in enumerate(comments, start=1):\n",
        "        print(f\"{idx}. {comment}\")\n",
        "\n",
        "    print(\"\\nSarcastic comments detected:\")\n",
        "    for idx, sarcastic_comment in enumerate(sarcastic_comments, start=1):\n",
        "        print(f\"{idx}. {sarcastic_comment}\")\n",
        "\n",
        "# Function to preprocess text (adjust based on your preprocessing steps)\n",
        "def preprocess_text(text):\n",
        "    # Example: lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char.isalnum() or char == ' '])  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Run the main function\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MN4XhNXMfEH",
        "outputId": "5bd478fb-20a5-4f4c-b900-5dfbcdef48e9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 comments:\n",
            "1. Namaskaram /u/shivamYe, Thank you for your submission. Please provide a source for the image / video (if not a direct link submission). We would really appreciate it if you could mention the source as a reply to this comment! If you have already provided the source or if it is an OC post, please ignore this message.  Thank you.\n",
            "\n",
            "\n",
            "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/IndiaSpeaks) if you have any questions or concerns.*\n",
            "2. ah yeah here wo go again\n",
            "3. Lol all those racist westerners are now cornered because if they protest about this, they'll just expose themselves\n",
            "4. [deleted]\n",
            "5. RaGa once said \"Maza aaya\"\n",
            "6. Elon just wanted to save X by promoting this controversy. This has increased user participation and retentivity.\n",
            "\n",
            "but good to see Indians stepping up for their country's image. feels good man.\n",
            "7. its crushing the dream of both gajwa e hind and communist occupation, those people have hearts too.\n",
            "8. i see users bringing this up, bjp should divert their it cell towards this.. lets annihilate the trend\n",
            "9. tell me something how come all these foreigners are ranting and abusing India all of a sudden, its like election ke pehle me X pe zyada hi foreigners ki baat dekh raha hu. Itne din kidhar the, bot s hain kya ye log?\n",
            "10. Gormint... der se aaye, magar...\n",
            "\n",
            "Sarcastic comments detected:\n",
            "1. [deleted]\n",
            "2. RaGa once said \"Maza aaya\"\n",
            "3. Elon just wanted to save X by promoting this controversy. This has increased user participation and retentivity.\n",
            "\n",
            "but good to see Indians stepping up for their country's image. feels good man.\n",
            "4. Gormint... der se aaye, magar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-07397c59c44d>:25: DeprecationWarning: Using CommentForest as an asynchronous iterator has been deprecated and will be removed in a future version.\n",
            "  comments = await fetch_reddit_post_comments(submission_url, limit=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hA4KYe9RSkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}